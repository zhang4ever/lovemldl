## 深度学习面试总结

1. **梯度消失和梯度爆炸的原因和解决方法**

   **原因：**

   BP的求导过程如下：

   ![](/Users/zhangbo/PycharmProjects/lovemldl/md/img/梯度消失原理.png)

   上式主要由两部分构成：**多个激活函数偏导数的连乘，和多个权重参数的连乘**。如果激活函数求导后与权重相乘的积大于1，那么随着层数增多，求出的梯度更新信息将以指数形式增加，即发生梯度爆炸；如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生梯度消失。

   **解决方法：**

   - **梯度剪切：**通过设置阈值，当梯度达到某一个阈值，则将其限制在某个范围内，防止爆炸。

   - **合适的参数初始化策略：**

     > - pre-training + fine-tuning 的方式来自动学习和更新需要的初始化参数
     >
     > - random initialization: np.random.randn(m,n), 缺点：随机分布选择不当，就会陷入困境
     >
     > - Xavier initialization: 想法保证输入和输出的方差一致。tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in)
     >
     >   适合比较适合tanh 
     >
     > - l He initialization：tf.Variable(np.random.randn(node_in,node_out))/np.sqrt(node_in/2)
     >
     >   比较适合ReLU

   - **合适的参数初始化：**relu, prelu等等

   - **Batch Normalization:** 本质是解决训练梯度问题，也具有加速网络训练速度、提升稳定性的作用。

     ![](/Users/zhangbo/PycharmProjects/lovemldl/md/img/BN.png)

     ​	连续多层BP的时候，$\frac{\partial f_l}{\partial f_k} = \prod_{i=k+1}^lW_i\frac{1}{\sigma_i}$,  而$ \frac{1}{\sigma_i} $ 能对$ W_i$ 进行缩放，从而防止梯度过小或者过大。

   - **使用残差网络**

     <img src="/Users/zhangbo/PycharmProjects/lovemldl/md/img/残差网络.png" alt="残差网络" style="zoom:70%;" />

     ​		每个残差块包括两部分，左侧的直接映射 + 右侧的残差（一般包含卷积操作），$h_l=h(x_l)+F(x_l,W_l)$, 

     $ x_{l+1} = f(y_l) $,从而 

     ​		$$ \frac{\partial \varepsilon}{\partial x_l}=\frac{\partial\varepsilon}{\partial x_L}\frac{\partial x_L}{\partial x_l}=\frac{\partial \varepsilon}{\partial x_L}(1+\frac{\partial}{\partial x_l}\sum_{i=1}^{L-1}F(x_i,W_i))=\frac{\partial \varepsilon}{\partial x_L}+\frac{\partial \varepsilon}{\partial x_L}\sum_{i=1}^{L-1}F(x_i,W_i) $$

     ​		由于$\frac{\partial \varepsilon}{\partial x_L}\sum_{i=1}^{L-1}F(x_i,W_i)$不可能一直为-1， 从而很大概率避免了导数为0的情况。

2. **LSTM 的结构是如何解决梯度消失和梯度爆炸的问题的**

   ![lstm](/Users/zhangbo/PycharmProjects/lovemldl/md/img/lstm.png)

   ​	                                               $f_t=\sigma(W_fX_t+b_f)$ ,    $i_t=\sigma(W_iX_t+b_i)$ ,        $o_i=\sigma(W_oX_t+b_o)$ 

   所以LSTM cell的状态为：$$ C_t=f_tC_{t-1}+i_tX_t$$ ，将其展开加上tan h激活函数后：

   ​			$$S_t=tanh[\sigma(W_fX_t+b_f)S_{t-1} + \sigma(W_iX_t+b_i)X_t]$$

   而传统的RNN BN时求偏导，结果为：

   ​				$$ \prod_{j=k+1}^t\frac{\partial S_j}{\partial S_{j-1}} = \prod_{j=k+1}^t tanh'W_s $$

   LSTM求偏导后也包含类似的过程，但是区别在于：

   ​				$$\prod_{j=k+1}^t\frac{\partial S_j}{\partial S_{j-1}} = \prod_{j=k+1}^t tanh' \sigma(
   W_fX_t+b_f)$$

   但是$\prod_{j=k+1}^t tanh' \sigma(
   W_fX_t+b_f)\approx0|1$.这从很大程度上就避免了

   RNN带来的梯度爆炸或者梯度消失的问题。

   - **拓展1：** **RNN的激活函数使用tanh而不是sigmoid或relu的原因？**

     Tanh的导数值域[0, 1],而sigmoid的导数值域[0, 0.25]，rnn会有多层循环结构，小于1的导数连乘会造成梯度消失；sigmoid的输出都是>=0，会出现偏移现象；使用relu再倒数连乘的时候容易发生梯度爆炸。

   - **拓展2:为什么cnn使用relu就可以？**

     Cnn不涉及循环结构，每个单元都是相对独立的，也就不存在梯度连乘。

3. ** **1\*1卷积核作用？**

   - **升维、降维（降低信道数）：** 一张500*500且厚度depth为100 的图片在20个filter上做1X1的卷积，那么结果的大小为500*500*20。

   - **加入非线性**。卷积层之后经过**激励层**，1X1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力；

   - 可以减少参数。具体例子参考 Inception网络的使用

4. **卷积的输出size计算公式？**

   output_size = (input_size - kernel_size + 2*padding)/ stride +1 

5. **CNN的特性？**

   - **局部连接：** 每次滑动窗口都是学习局部的特征

   - **权值共享：** 每次滑动时，卷积核上的参数都是一样的

   - **池化操作：**特征图压缩、特征提取、减少计算量

     **Max-pooling:** 特征选择，选出了分类辨识度更好的特征，提供了非线性，在网络比较深的时候，能更好的将主要的特征传递下去。

     **Average-pooling:** 整体特征信息下采样，偏向于信息完整性的传递。

6. 

7. 

   